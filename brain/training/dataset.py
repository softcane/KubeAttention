"""
PyTorch Dataset for KubeAttention training.

Loads scheduling events from Parquet files and converts them to
tensors suitable for training the AttentionScorer model.
"""

import json
from pathlib import Path
from typing import Optional, Tuple
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader

try:
    import pyarrow.parquet as pq
    HAS_PARQUET = True
except ImportError:
    HAS_PARQUET = False

# Import feature definitions from single source of truth
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))
from brain.metrics_schema import FEATURE_NAMES, TETRAGON_METRICS_SCHEMA
from brain.tensor_encoder import PodContext
from brain.config import DATASET, CLUSTER, TRAINING


class SchedulingDataset(Dataset):
    """
    Dataset of scheduling decisions with outcomes.
    
    Each sample contains:
    - node_features: (N, T, F) telemetry for all candidate nodes
    - pod_context: (P,) pod requirements
    - label: index of the best node (based on outcome)
    - weight: importance weight based on outcome severity
    
    ============================================================================
    DATA SOURCES
    ============================================================================
    
    1. PRODUCTION DATA (Recommended):
       - Collected automatically by the Collector (pkg/collector/collector.go)
       - The Collector watches scheduling events and writes JSONL to disk
       - Use: train_model(train_data_path="/path/to/collector/output.jsonl")
    
    2. SYNTHETIC DATA (Bootstrapping Only):
       - Generated by generate_synthetic_data() for cold-start and CI/CD
       - WARNING: Synthetic data is a simplified model. Always train on real
         cluster data for production deployments!
       
    To collect real data:
       1. Deploy the Collector: kubectl apply -f deploy/collector.yaml
       2. Wait for scheduling events to accumulate
       3. Export: kubectl cp collector-pod:/data/events.jsonl ./training_data.jsonl
       4. Train: python -m brain.training.train --train-data training_data.jsonl
    ============================================================================
    """
    
    # Feature names from metrics_schema.py (single source of truth)
    # DO NOT redefine here - import from metrics_schema to avoid mismatch!
    
    # Outcome to label mapping and weights now managed in config.py
    # Access via TRAINING.OUTCOME_LABELS and TRAINING.OUTCOME_WEIGHTS

    
    def __init__(
        self,
        data_path: str,
        max_nodes: int = None,  # Use None for dynamic sizing
        temporal_window: int = None,  # Use config default
    ):
        self.data_path = Path(data_path)
        # Use config defaults if not specified
        self.max_nodes = max_nodes or DATASET.MAX_NODES
        self.temporal_window = temporal_window or DATASET.TEMPORAL_WINDOW
        
        # Load data
        self.events = self._load_data()
        
        print(f"Loaded {len(self.events)} scheduling events from {data_path}")
    
    def _load_data(self) -> list:
        """Load events from JSONL or Parquet file."""
        events = []
        
        if self.data_path.suffix == ".parquet" and HAS_PARQUET:
            table = pq.read_table(self.data_path)
            df = table.to_pandas()
            for _, row in df.iterrows():
                events.append(row.to_dict())
        elif self.data_path.suffix == ".jsonl":
            with open(self.data_path) as f:
                for line in f:
                    if line.strip():
                        events.append(json.loads(line))
        else:
            raise ValueError(f"Unsupported file format: {self.data_path.suffix}")
        
        # Filter events with valid outcomes
        events = [e for e in events if e.get("outcome") != "pending"]
        
        return events
    
    def __len__(self) -> int:
        return len(self.events)
    
    def __getitem__(self, idx: int) -> dict:
        event = self.events[idx]
        
        # Extract node telemetry
        node_telemetry = event.get("node_telemetry", {})
        node_names = list(node_telemetry.keys())[:self.max_nodes]
        num_nodes = len(node_names)
        
        # Build node features tensor: (N, T, F)
        node_features = np.zeros((self.max_nodes, self.temporal_window, len(FEATURE_NAMES)))
        
        for i, node_name in enumerate(node_names):
            metrics = node_telemetry[node_name]
            for j, feature_name in enumerate(FEATURE_NAMES):
                value = metrics.get(feature_name, 0.0)
                # Normalize to [0, 1] range
                # Fill all temporal snapshots with current value for single-shot data
                node_features[i, :, j] = self._normalize_feature(feature_name, value)
        
        # Build pod context using the shared PodContext class (Issue 10 fix: unify encoding)
        pod = PodContext(
            pod_name=event.get("pod_name", ""),
            pod_namespace=event.get("pod_namespace", ""),
            cpu_milli=event.get("cpu_request_milli", 0),
            memory_bytes=event.get("memory_request_bytes", 0),
            workload_type=event.get("workload_type", "unknown"),
            criticality=event.get("criticality", "unknown"),
            labels=event.get("pod_labels", {})
        )
        pod_context = np.array(pod.to_feature_vector(), dtype=np.float32)
        
        # Create label: which node was chosen
        chosen_node = event.get("chosen_node", "")
        chosen_idx = node_names.index(chosen_node) if chosen_node in node_names else 0
        
        # Create binary labels for all nodes based on outcome
        outcome = event.get("outcome", "unknown")
        outcome_score = self.OUTCOME_LABELS.get(outcome, 0.5)
        
        # LABEL LOGIC IMPROVEMENT:
        # Instead of sparse labels (only chosen node), we use the outcome score 
        # as a baseline and then adjust other nodes based on their relative telemetry.
        # This gives a much richer training signal.
        labels = np.zeros(self.max_nodes, dtype=np.float32)
        
        # Ground truth: nodes with better metrics should have higher scores
        for i, node_name in enumerate(node_names):
            metrics = node_telemetry[node_name]
            # Heuristic calculation using config weights
            cpu = metrics.get("cpu_utilization", 0.5)
            mem = metrics.get("memory_utilization", 0.5)
            cache = metrics.get("l3_cache_miss_rate", 0.1)
            
            # Score is in [0, 1]
            node_score = (
                (1.0 - cpu) * TRAINING.CPU_WEIGHT + 
                (1.0 - mem) * TRAINING.MEM_WEIGHT + 
                (1.0 - cache) * TRAINING.CACHE_WEIGHT
            )
            
            # If this was the chosen node, factor in the ACTUAL outcome
            if node_name == chosen_node:
                # Weighted average of telemetry and outcome
                node_score = (
                    TRAINING.TELEMETRY_TRUST_FACTOR * node_score + 
                    TRAINING.OUTCOME_TRUST_FACTOR * outcome_score
                )
            
            labels[i] = node_score
        
        # Attention mask: 1 for real nodes, 0 for padding
        attention_mask = np.zeros(self.max_nodes, dtype=np.float32)
        attention_mask[:num_nodes] = 1.0
        
        # Sample weight based on outcome
        # Sample weight based on outcome
        weight = TRAINING.OUTCOME_WEIGHTS.get(outcome, 1.0)
        
        return {
            "node_features": torch.tensor(node_features, dtype=torch.float32),
            "pod_context": torch.tensor(pod_context, dtype=torch.float32),
            "labels": torch.tensor(labels, dtype=torch.float32),
            "attention_mask": torch.tensor(attention_mask, dtype=torch.float32),
            "weight": torch.tensor(weight, dtype=torch.float32),
            "chosen_idx": torch.tensor(chosen_idx, dtype=torch.long),
            "num_nodes": torch.tensor(num_nodes, dtype=torch.long),
        }
    
    def _normalize_feature(self, feature_name: str, value: float) -> float:
        """Normalize a feature to [0, 1] range using TETRAGON_METRICS_SCHEMA."""
        # Use single source of truth for normalization ranges
        for spec in TETRAGON_METRICS_SCHEMA:
            if spec.name == feature_name:
                normalized = (value - spec.min_value) / (spec.max_value - spec.min_value + 1e-8)
                return max(0.0, min(1.0, normalized))
        # Fallback for unknown features
        return max(0.0, min(1.0, value))


def create_dataloader(
    data_path: str,
    batch_size: int = 32,
    shuffle: bool = True,
    num_workers: int = 4,
) -> DataLoader:
    """Create a DataLoader for training."""
    dataset = SchedulingDataset(data_path)
    return DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        num_workers=num_workers,
        pin_memory=True,
    )


def generate_synthetic_data(output_path: str, num_samples: int = None):
    """
    Generate synthetic bootstrapping data for cold-start or CI/CD.
    
    WARNING: For production use, always train on real data collected from 
    your cluster via the Collector and Shadow Mode. Synthetic data is 
    a simplified model used for verification and bootstrapping only.
    
    To collect REAL training data:
        1. Deploy the Collector: kubectl apply -f deploy/collector.yaml
        2. Let it run for 24-48 hours to gather scheduling events
        3. Export: kubectl cp collector-pod:/data/events.jsonl ./training_data.jsonl
        4. Train: python -m brain.training.train --train-data training_data.jsonl
    """
    import random
    from datetime import datetime, timedelta
    
    # Use config defaults
    num_samples = num_samples or DATASET.DEFAULT_NUM_SAMPLES
    num_nodes = DATASET.DEFAULT_NUM_NODES
    
    events = []
    
    node_names = [f"node-{i}" for i in range(num_nodes)]
    
    # Use central config for zones (no hardcoding!)
    availability_zones = CLUSTER.AVAILABILITY_ZONES
    
    for i in range(num_samples):
        # Random pod requirements
        cpu_milli = random.randint(100, 4000)
        memory_bytes = random.randint(128 * 1024**2, 16 * 1024**3)
        
        # Dynamic timestamp (spread over last 30 days for realistic training)
        event_time = datetime.now() - timedelta(
            days=random.randint(0, 30),
            hours=random.randint(0, 23),
            minutes=random.randint(0, 59)
        )
        timestamp_str = event_time.isoformat() + "Z"
        outcome_time = event_time + timedelta(minutes=5)
        outcome_timestamp_str = outcome_time.isoformat() + "Z"
        
        # Generate telemetry for each node
        node_telemetry = {}
        best_node = None
        best_score = -1
        
        for node_name in node_names:
            # Random utilization
            cpu_util = random.uniform(0.1, 0.9)
            mem_util = random.uniform(0.1, 0.9)
            cache_miss = random.uniform(0.0, 0.5)
            
            # Simple scoring: lower utilization = better
            score = (1 - cpu_util) * 0.4 + (1 - mem_util) * 0.4 + (1 - cache_miss) * 0.2
            
            if score > best_score:
                best_score = score
                best_node = node_name
            
            # Random metadata for Phase 2 & 4 (using config)
            cost_idx = random.choice([0.01, 0.05, 0.1, 0.2, 0.4, 0.6, 0.8, 1.0])
            is_spot = random.random() < 0.3
            zone = random.choice(availability_zones)  # From central config
            spot_risk = random.uniform(0, 0.1) if is_spot else 0.0
            
            node_telemetry[node_name] = {
                "node_name": node_name,
                "timestamp": timestamp_str,
                "cpu_utilization": cpu_util,
                "memory_utilization": mem_util,
                "l3_cache_miss_rate": cache_miss,
                "l3_cache_occupancy_mb": random.uniform(0, 64),
                "memory_bandwidth_gbps": random.uniform(0, 50),
                "disk_io_wait_ms": random.uniform(0, 100),
                "disk_iops": random.uniform(0, 10000),
                "network_rx_packets_sec": random.uniform(0, 100000),
                "network_tx_packets_sec": random.uniform(0, 100000),
                "network_drop_rate": random.uniform(0, 0.01),
                "cpu_throttle_rate": random.uniform(0, 100),
                "node_cost_index": cost_idx,
                "is_spot_instance": is_spot,
                "availability_zone": zone,
                "spot_interruption_risk": spot_risk,
            }
        
        # Simulate outcome based on placement quality
        # If placed on best node: high success rate
        # If placed on bad node: high failure rate
        chosen_node = random.choice(node_names)
        was_optimal = (chosen_node == best_node)
        
        if was_optimal:
            outcome = random.choices(
                ["running", "restarted", "oom_killed"],
                weights=[0.95, 0.04, 0.01]
            )[0]
        else:
            outcome = random.choices(
                ["running", "restarted", "oom_killed", "evicted"],
                weights=[0.1, 0.3, 0.3, 0.3]
            )[0]
        
        events.append({
            "timestamp": timestamp_str,
            "event_id": f"synthetic-{i}",
            "pod_uid": f"pod-{i}",
            "pod_name": f"test-pod-{i}",
            "pod_namespace": "default",
            "pod_labels": {"app": "synthetic"},
            "cpu_request_milli": cpu_milli,
            "memory_request_bytes": memory_bytes,
            "candidate_nodes": node_names,
            "node_telemetry": node_telemetry,
            "chosen_node": chosen_node,
            "scheduler_name": "default-scheduler",
            "outcome": outcome,
            "outcome_timestamp": outcome_timestamp_str,
            "p99_latency_ms": random.uniform(1, 100),
        })
    
    # Write to JSONL
    with open(output_path, 'w') as f:
        for event in events:
            f.write(json.dumps(event) + '\n')
    
    print(f"Generated {num_samples} synthetic training samples to {output_path}")
    return output_path


if __name__ == "__main__":
    # Generate synthetic data for testing
    generate_synthetic_data("training_data.jsonl", 10000)
    
    # Test dataset loading
    dataset = SchedulingDataset("training_data.jsonl")
    sample = dataset[0]
    print("Sample keys:", sample.keys())
    print("Node features shape:", sample["node_features"].shape)
    print("Pod context shape:", sample["pod_context"].shape)
